{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6b012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bec0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.lineage import context, artifact, association, action\n",
    "import boto3\n",
    "\n",
    "from model_package_src.inference_specification import InferenceSpecification\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from scipy.sparse import csr_matrix, hstack, load_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb3948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e4675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sagemaker.__version__ >= \"2.21.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "020af405",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'quicksight-pavan-cjp'\n",
    "\n",
    "prefix = \"personalization\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3a4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data For Modeling\n",
    "# load array\n",
    "X_train = load_npz(\"./data/X_train.npz\")\n",
    "X_test = load_npz(\"./data/X_test.npz\")\n",
    "y_train_npzfile = np.load(\"./data/y_train.npz\")\n",
    "y_test_npzfile = np.load(\"./data/y_test.npz\")\n",
    "y_train = y_train_npzfile.f.arr_0\n",
    "y_test = y_test_npzfile.f.arr_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c1a44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219519, 9284), (54880, 9284), (219519,), (54880,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208b113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'input_dims' (int)\n"
     ]
    }
   ],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "%store input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f3591f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the factorization machine model\n",
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=boto_session.region_name)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=input_dims,\n",
    "    predictor_type=\"regressor\",\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe98e21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-25 09:54:06 Starting - Starting the training job...\n",
      "2022-03-25 09:54:25 Starting - Preparing the instances for trainingProfilerReport-1648202046: InProgress\n",
      "......\n",
      "2022-03-25 09:55:29 Downloading - Downloading input data...\n",
      "2022-03-25 09:56:01 Training - Downloading the training image...\n",
      "2022-03-25 09:56:35 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '20', 'feature_dim': '9284', 'mini_batch_size': '1000', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Final configuration: {'epochs': '20', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '9284', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 WARNING 139646129960768] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Using default worker.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:39.506] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:39.514] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 99840}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] nvidia-smi: took 0.042 seconds to run.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202199.5009897, \"EndTime\": 1648202199.5626433, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 51.671743392944336, \"count\": 1, \"min\": 51.671743392944336, \"max\": 51.671743392944336}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202199.562789, \"EndTime\": 1648202199.56283, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[09:56:39] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206299.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[09:56:39] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206299.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=72.22661905419636\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=5216.6845\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:39 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=17.63289453125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:42.667] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 3064, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=91.21242742344528\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, train mse <loss>=8319.706916477273\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=16.162756942471592\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202199.5627372, \"EndTime\": 1648202202.6681523, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 3105.043649673462, \"count\": 1, \"min\": 3105.043649673462, \"max\": 3105.043649673462}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202199.5630755, \"EndTime\": 1648202202.668409, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 220519.0, \"count\": 1, \"min\": 220519, \"max\": 220519}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=70688.86696867847 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=70.0600956322499\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=4908.417\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:42 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=17.41325390625\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:45.403] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2731, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=90.60802512809579\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, train mse <loss>=8209.814217613637\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=18.42056477716619\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202202.6682503, \"EndTime\": 1648202205.4039025, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2733.982801437378, \"count\": 1, \"min\": 2733.982801437378, \"max\": 2733.982801437378}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202202.669896, \"EndTime\": 1648202205.404059, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 440038.0, \"count\": 1, \"min\": 440038, \"max\": 440038}, \"Total Batches Seen\": {\"sum\": 441.0, \"count\": 1, \"min\": 441, \"max\": 441}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=80285.1404579708 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=69.8893983376592\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=4884.528\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:45 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=18.55230078125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:48.110] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2704, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=90.47375148602242\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, train mse <loss>=8185.499707954546\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=18.580566246448864\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202205.4039576, \"EndTime\": 1648202208.11139, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2706.386089324951, \"count\": 1, \"min\": 2706.386089324951, \"max\": 2706.386089324951}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202205.4049785, \"EndTime\": 1648202208.1116292, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 659557.0, \"count\": 1, \"min\": 659557, \"max\": 659557}, \"Total Batches Seen\": {\"sum\": 661.0, \"count\": 1, \"min\": 661, \"max\": 661}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=81100.03434120417 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=69.72750533326142\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=4861.925\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:48 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=18.33094140625\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:50.832] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2717, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=90.33190701046739\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, train mse <loss>=8159.853424147727\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=18.27153690962358\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202208.1114597, \"EndTime\": 1648202210.83259, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2719.393014907837, \"count\": 1, \"min\": 2719.393014907837, \"max\": 2719.393014907837}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202208.1131759, \"EndTime\": 1648202210.8327682, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 879076.0, \"count\": 1, \"min\": 879076, \"max\": 879076}, \"Total Batches Seen\": {\"sum\": 881.0, \"count\": 1, \"min\": 881, \"max\": 881}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=80714.20892661685 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=69.57381332081778\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=4840.5155\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:50 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=18.07842578125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:53.521] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2685, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=90.2005097842629\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, train mse <loss>=8136.131965340909\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=18.01931514115767\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202210.832641, \"EndTime\": 1648202213.521667, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2687.798261642456, \"count\": 1, \"min\": 2687.798261642456, \"max\": 2687.798261642456}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202210.8338468, \"EndTime\": 1648202213.5218103, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1098595.0, \"count\": 1, \"min\": 1098595, \"max\": 1098595}, \"Total Batches Seen\": {\"sum\": 1101.0, \"count\": 1, \"min\": 1101, \"max\": 1101}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=81664.93397219223 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=69.4391388195447\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=4821.794\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:53 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=17.880751953125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:56.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2778, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=90.08360983045266\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, train mse <loss>=8115.056760085227\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=17.845939994673294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202213.5217168, \"EndTime\": 1648202216.3034978, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2780.5190086364746, \"count\": 1, \"min\": 2780.5190086364746, \"max\": 2780.5190086364746}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202213.5229554, \"EndTime\": 1648202216.3037324, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1318114.0, \"count\": 1, \"min\": 1318114, \"max\": 1318114}, \"Total Batches Seen\": {\"sum\": 1321.0, \"count\": 1, \"min\": 1321, \"max\": 1321}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=78938.34361971024 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=69.32097085298214\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=4805.397\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:56 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=17.763322265625\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:56:58.978] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2664, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:58 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=89.97837475042141\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:58 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, train mse <loss>=8096.107922727273\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:58 INFO 139646129960768] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=17.719223974609374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202216.3035467, \"EndTime\": 1648202218.9789667, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2666.438102722168, \"count\": 1, \"min\": 2666.438102722168, \"max\": 2666.438102722168}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:58 INFO 139646129960768] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202216.312504, \"EndTime\": 1648202218.9791112, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1537633.0, \"count\": 1, \"min\": 1537633, \"max\": 1537633}, \"Total Batches Seen\": {\"sum\": 1541.0, \"count\": 1, \"min\": 1541, \"max\": 1541}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:58 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=82318.64368529695 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:59 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=69.21636367218376\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:59 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=4790.905\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:56:59 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=17.67858984375\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:01.769] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 2787, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=89.88232149183378\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, train mse <loss>=8078.831716761364\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=17.620570072798294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202218.9790182, \"EndTime\": 1648202221.7737129, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2793.3928966522217, \"count\": 1, \"min\": 2793.3928966522217, \"max\": 2793.3928966522217}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202218.9802978, \"EndTime\": 1648202221.7738752, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1757152.0, \"count\": 1, \"min\": 1757152, \"max\": 1757152}, \"Total Batches Seen\": {\"sum\": 1761.0, \"count\": 1, \"min\": 1761, \"max\": 1761}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=78577.31888929171 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=69.12171511182285\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=4777.8115\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:01 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=17.6106328125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:04.607] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2830, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=89.79394862019834\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, train mse <loss>=8062.953208806818\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=17.54735018643466\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202221.7737672, \"EndTime\": 1648202224.608309, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2833.008289337158, \"count\": 1, \"min\": 2833.008289337158, \"max\": 2833.008289337158}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202221.7752717, \"EndTime\": 1648202224.6084495, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1976671.0, \"count\": 1, \"min\": 1976671, \"max\": 1976671}, \"Total Batches Seen\": {\"sum\": 1981.0, \"count\": 1, \"min\": 1981, \"max\": 1981}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=77478.17643860183 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=69.03420166844838\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=4765.721\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:04 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=17.550154296875\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:07.754] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 3143, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=89.71251026638066\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, train mse <loss>=8048.334498295455\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=17.494896257990057\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202224.6083584, \"EndTime\": 1648202227.7556233, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3146.0323333740234, \"count\": 1, \"min\": 3146.0323333740234, \"max\": 3146.0323333740234}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202224.6095667, \"EndTime\": 1648202227.7558732, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2196190.0, \"count\": 1, \"min\": 2196190, \"max\": 2196190}, \"Total Batches Seen\": {\"sum\": 2201.0, \"count\": 1, \"min\": 2201, \"max\": 2201}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=69767.23929815202 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=68.95210294109962\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=4754.3925\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:07 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=17.490455078125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:10.521] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2763, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=89.63715438616323\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, train mse <loss>=8034.819446448863\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=17.45398817471591\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202227.7557166, \"EndTime\": 1648202230.5222151, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2765.016794204712, \"count\": 1, \"min\": 2765.016794204712, \"max\": 2765.016794204712}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202227.757174, \"EndTime\": 1648202230.5224297, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2415709.0, \"count\": 1, \"min\": 2415709, \"max\": 2415709}, \"Total Batches Seen\": {\"sum\": 2421.0, \"count\": 1, \"min\": 2421, \"max\": 2421}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=79381.64987818967 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=68.87503901995265\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=4743.771\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:10 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=17.436017578125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:13.266] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2742, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=89.56761664950523\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, train mse <loss>=8022.357952272727\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=17.42097587890625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202230.52228, \"EndTime\": 1648202233.267327, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2743.828058242798, \"count\": 1, \"min\": 2743.828058242798, \"max\": 2743.828058242798}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202230.5234787, \"EndTime\": 1648202233.2674654, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2635228.0, \"count\": 1, \"min\": 2635228, \"max\": 2635228}, \"Total Batches Seen\": {\"sum\": 2641.0, \"count\": 1, \"min\": 2641, \"max\": 2641}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=79997.65582088813 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=68.80243091635644\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=4733.7745\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:13 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=17.39027734375\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:16.126] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2856, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=89.502190183175\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, train mse <loss>=8010.642047585227\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=17.38904547674006\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202233.2673748, \"EndTime\": 1648202236.126472, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2857.7561378479004, \"count\": 1, \"min\": 2857.7561378479004, \"max\": 2857.7561378479004}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202233.2686932, \"EndTime\": 1648202236.126621, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2854747.0, \"count\": 1, \"min\": 2854747, \"max\": 2854747}, \"Total Batches Seen\": {\"sum\": 2861.0, \"count\": 1, \"min\": 2861, \"max\": 2861}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=76808.05530136751 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=68.73474376179779\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=4724.465\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:16 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=17.34413671875\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:18.913] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2784, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=89.44082671374765\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, train mse <loss>=7999.661483238637\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=17.362610808771308\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202236.1265235, \"EndTime\": 1648202238.9140868, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2786.0543727874756, \"count\": 1, \"min\": 2786.0543727874756, \"max\": 2786.0543727874756}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202236.1280103, \"EndTime\": 1648202238.9142187, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3074266.0, \"count\": 1, \"min\": 3074266, \"max\": 3074266}, \"Total Batches Seen\": {\"sum\": 3081.0, \"count\": 1, \"min\": 3081, \"max\": 3081}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=78785.21645484529 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=68.67233795350207\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=4715.89\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:18 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=17.292935546875\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:21.703] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2787, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=89.38316585868087\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, train mse <loss>=7989.350338920454\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=17.333876198508523\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202238.9141295, \"EndTime\": 1648202241.7040102, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2788.813829421997, \"count\": 1, \"min\": 2788.813829421997, \"max\": 2788.813829421997}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202238.9151738, \"EndTime\": 1648202241.7041605, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3293785.0, \"count\": 1, \"min\": 3293785, \"max\": 3293785}, \"Total Batches Seen\": {\"sum\": 3301.0, \"count\": 1, \"min\": 3301, \"max\": 3301}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=78706.83003996764 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=68.61341705526696\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=4707.801\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:21 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=17.2486875\u001b[0m\n",
      "\n",
      "2022-03-25 09:57:37 Uploading - Uploading generated training model\u001b[34m[2022-03-25 09:57:24.399] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2691, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=89.32885107497465\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, train mse <loss>=7979.643634375\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=17.30803719815341\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202241.7040617, \"EndTime\": 1648202244.40004, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2694.6091651916504, \"count\": 1, \"min\": 2694.6091651916504, \"max\": 2694.6091651916504}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202241.7054086, \"EndTime\": 1648202244.400186, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3513304.0, \"count\": 1, \"min\": 3513304, \"max\": 3513304}, \"Total Batches Seen\": {\"sum\": 3521.0, \"count\": 1, \"min\": 3521, \"max\": 3521}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=81458.19035853198 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=68.55855161830652\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=4700.275\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:24 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=17.207400390625\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:27.111] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2709, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=89.2773943729471\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, train mse <loss>=7970.453146022727\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=17.283455996981534\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202244.4000888, \"EndTime\": 1648202247.1123462, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2711.1153602600098, \"count\": 1, \"min\": 2711.1153602600098, \"max\": 2711.1153602600098}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202244.4012046, \"EndTime\": 1648202247.1125727, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3732823.0, \"count\": 1, \"min\": 3732823, \"max\": 3732823}, \"Total Batches Seen\": {\"sum\": 3741.0, \"count\": 1, \"min\": 3741, \"max\": 3741}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=80958.83249399225 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=68.50716750822501\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=4693.232\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:27 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=17.1710078125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:29.768] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2652, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=89.22836755916096\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, train mse <loss>=7961.701577272727\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=17.25918828568892\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202247.112417, \"EndTime\": 1648202249.7684586, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2654.3118953704834, \"count\": 1, \"min\": 2654.3118953704834, \"max\": 2654.3118953704834}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202247.1141229, \"EndTime\": 1648202249.7686033, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3952342.0, \"count\": 1, \"min\": 3952342, \"max\": 3952342}, \"Total Batches Seen\": {\"sum\": 3961.0, \"count\": 1, \"min\": 3961, \"max\": 3961}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=82694.51561450156 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=68.45944785053412\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=4686.696\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:29 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=17.1378046875\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:32.529] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2758, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=89.18173190559723\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, train mse <loss>=7953.381305681818\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=17.23677677112926\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202249.7685094, \"EndTime\": 1648202252.5297348, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2759.8464488983154, \"count\": 1, \"min\": 2759.8464488983154, \"max\": 2759.8464488983154}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202249.769865, \"EndTime\": 1648202252.5298808, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4171861.0, \"count\": 1, \"min\": 4171861, \"max\": 4171861}, \"Total Batches Seen\": {\"sum\": 4181.0, \"count\": 1, \"min\": 4181, \"max\": 4181}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=79532.95681646148 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=68.41493988888685\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=4680.604\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:32 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=17.10520703125\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:35.231] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2698, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202252.5297842, \"EndTime\": 1648202255.2321613, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2700.9825706481934, \"count\": 1, \"min\": 2700.9825706481934, \"max\": 2700.9825706481934}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202252.5311525, \"EndTime\": 1648202255.2323294, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4391380.0, \"count\": 1, \"min\": 4391380, \"max\": 4391380}, \"Total Batches Seen\": {\"sum\": 4401.0, \"count\": 1, \"min\": 4401, \"max\": 4401}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #throughput_metric: host=algo-1, train throughput=81264.76123875065 records/second\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 WARNING 139646129960768] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202255.2322123, \"EndTime\": 1648202255.2359068, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 3.329038619995117, \"count\": 1, \"min\": 3.329038619995117, \"max\": 3.329038619995117}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] Saved checkpoint to \"/tmp/tmp3xuo029u/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:35.250] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 55743, \"num_examples\": 1, \"num_bytes\": 99820}\u001b[0m\n",
      "\u001b[34m[2022-03-25 09:57:35.495] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 244, \"num_examples\": 55, \"num_bytes\": 5462876}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202255.2502236, \"EndTime\": 1648202255.4953105, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Total Batches Seen\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Max Records Seen Between Resets\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Max Batches Seen Between Resets\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Number of Batches Since Last Reset\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #test_score (algo-1) : ('rmse', 68.74431145296938)\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #test_score (algo-1) : ('mse', 4725.780357142857)\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #test_score (algo-1) : ('absolute_loss', 17.043618124914587)\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, test rmse <loss>=68.74431145296938\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, test mse <loss>=4725.780357142857\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768] #quality_metric: host=algo-1, test absolute_loss <loss>=17.043618124914587\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1648202255.2359767, \"EndTime\": 1648202255.4965384, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 21.477699279785156, \"count\": 1, \"min\": 21.477699279785156, \"max\": 21.477699279785156}, \"totaltime\": {\"sum\": 56023.30279350281, \"count\": 1, \"min\": 56023.30279350281, \"max\": 56023.30279350281}}}\u001b[0m\n",
      "\u001b[34m[03/25/2022 09:57:35 INFO 139646129960768 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-03-25 09:58:02 Completed - Training job completed\n",
      "Training seconds: 139\n",
      "Billable seconds: 139\n",
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "if 'training_job_name' not in locals():\n",
    "    \n",
    "    fm.fit({'train': 's3://quicksight-pavan-cjp/personalization/train/train.protobuf', 'test': 's3://quicksight-pavan-cjp/personalization/test/test.protobuf'})\n",
    "    training_job_name = fm.latest_training_job.job_name\n",
    "    %store training_job_name\n",
    "    \n",
    "else:\n",
    "    print(f'Using previous training job: {training_job_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361ba665",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06b643d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:519852036875:artifact/6621cbd0d7bf919399afbdeab7a40571\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_info[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=training_data_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {training_data_artifact.artifact_arn}\")\n",
    "else:\n",
    "    training_data_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainingData\",\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type=\"Dataset\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "973fdd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:519852036875:artifact/4c8348d93e65cf398e8a98333c7a9b41\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=trained_model_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {model_artifact.artifact_arn}\")\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainedModel\",\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type=\"Model\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {model_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5177620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set artifact associations\n",
    "trial_component = sagemaker_boto_client.describe_trial_component(\n",
    "    TrialComponentName=training_job_name + \"-aws-training-job\"\n",
    ")\n",
    "trial_component_arn = trial_component[\"TrialComponentArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f4454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists with DataSet\n",
      "Association with Model: SUCCEESFUL\n"
     ]
    }
   ],
   "source": [
    "artifact_list = [[training_data_artifact, \"ContributedTo\"], [model_artifact, \"Produced\"]]\n",
    "\n",
    "for art, assoc in artifact_list:\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=art.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Association with {art.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {art.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b763503a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model retail-recommendations already exists.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"retail-recommendations\"\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "\n",
    "if not model_matches:\n",
    "    print(f\"Creating model {model_name}\")\n",
    "    model = sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=training_job_info[\"TrainingJobName\"],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c9b6f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mpg_name' (str)\n",
      "Model Package Group name: retail-recommendation-2022-03-25-09-58\n"
     ]
    }
   ],
   "source": [
    "#SageMaker Model Registry\n",
    "#Create Model Package Group\n",
    "if 'mpg_name' not in locals():\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    mpg_name = f'retail-recommendation-{timestamp}'\n",
    "    %store mpg_name\n",
    "\n",
    "print(f'Model Package Group name: {mpg_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a05ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageGroupDescription\": \"Recommendation for Online Retail Sales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4da37896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model Package Group retail-recommendation-2022-03-25-09-58: SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)[\n",
    "    \"ModelPackageGroupSummaryList\"\n",
    "]\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f\"Using existing Model Package Group: {mpg_name}\")\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f\"Create Model Package Group {mpg_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6817baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {\"regression_metrics\": {}}\n",
    "\n",
    "for metric in training_job_info[\"FinalMetricDataList\"]:\n",
    "    stat = {metric[\"MetricName\"]: {\"value\": metric[\"Value\"]}}\n",
    "    model_metrics_report[\"regression_metrics\"].update(stat)\n",
    "\n",
    "with open(\"training_metrics.json\", \"w\") as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "\n",
    "metrics_s3_key = f\"training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename=\"training_metrics.json\",Bucket=bucket, Key=metrics_s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c341c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=[\"application/x-recordio-protobuf\", \"application/json\"],\n",
    "    supported_mime_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f61cfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    \"ModelQuality\": {\n",
    "        \"Statistics\": {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"S3Uri\": f\"s3://{bucket}/{metrics_s3_key}\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d7a31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageDescription\": \"Factorization Machine Model to create personalized retail recommendations\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    \"ModelMetrics\": model_metrics,\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bfaad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model package status: Completed\n"
     ]
    }
   ],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    ")\n",
    "mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "\n",
    "while mp_status not in [\"Completed\", \"Failed\"]:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(\n",
    "        ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    "    )\n",
    "    mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "    print(f\"model package status: {mp_status}\")\n",
    "print(f\"model package status: {mp_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85347298",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)[\n",
    "    \"ModelPackageSummaryList\"\n",
    "][0]\n",
    "model_package_update = {\n",
    "    \"ModelPackageArn\": model_package[\"ModelPackageArn\"],\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae17fee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...2-03-25-09-54-06-616/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...n-cjp/personalization/test/test.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...cjp/personalization/train/train.protobuf</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38241...1.amazonaws.com/factorization-machines:1</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...2-03-25-09-54-06-616/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...2-03-25-09-54-06-616/output/model.tar.gz     Input    Model   \n",
       "1  s3://...n-cjp/personalization/test/test.protobuf     Input  DataSet   \n",
       "2  s3://...cjp/personalization/train/train.protobuf     Input  DataSet   \n",
       "3  38241...1.amazonaws.com/factorization-machines:1     Input    Image   \n",
       "4  s3://...2-03-25-09-54-06-616/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0         Produced     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker_session)\n",
    "display(viz.show(training_job_name=training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68c4416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c140df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {\"instances\": []}\n",
    "        for row in data:\n",
    "            js[\"instances\"].append({\"features\": row.tolist()})\n",
    "        return json.dumps(js)\n",
    "\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80f6419e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer ID of highest spend: 14646.0\n"
     ]
    }
   ],
   "source": [
    "# find customer who spent the most money\n",
    "df = pd.read_csv(\"data/online_retail_preprocessed.csv\")\n",
    "\n",
    "df[\"invoice_amount\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "top_customer = (\n",
    "    df.groupby(\"CustomerID\").sum()[\"invoice_amount\"].sort_values(ascending=False).index[0]\n",
    ")\n",
    "print(\"Customer ID of highest spend:\", top_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "106bcec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, customer_id, n_recommendations, n_ranks=100):\n",
    "    popular_items = (\n",
    "        df.groupby([\"StockCode\", \"UnitPrice\"])\n",
    "        .nunique()[\"CustomerID\"]\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    top_n_items = popular_items[\"StockCode\"].iloc[:n_ranks].values\n",
    "    top_n_prices = popular_items[\"UnitPrice\"].iloc[:n_ranks].values\n",
    "\n",
    "    # stock codes can have multiple descriptions, so we will choose whichever description is most common\n",
    "    item_map = df.groupby(\"StockCode\").agg(lambda x: x.value_counts().index[0])[\"Description\"]\n",
    "\n",
    "    # find customer's country\n",
    "    df_subset = df.loc[df[\"CustomerID\"] == customer_id]\n",
    "    country = df_subset[\"Country\"].value_counts().index[0]\n",
    "\n",
    "    data = {\n",
    "        \"StockCode\": top_n_items,\n",
    "        \"Description\": [item_map[i] for i in top_n_items],\n",
    "        \"CustomerID\": customer_id,\n",
    "        \"Country\": country,\n",
    "        \"UnitPrice\": top_n_prices,\n",
    "    }\n",
    "\n",
    "    df_inference = pd.DataFrame(data)\n",
    "\n",
    "    # we need to build the data set similar to how we built it for training\n",
    "    # it should have the same number of features as the training data\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    enc.fit(df[onehot_cols])\n",
    "    onehot_output = enc.transform(df_inference[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = df[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(df_inference[\"Description\"])\n",
    "\n",
    "    row = range(len(df_inference))\n",
    "    col = [0] * len(df_inference)\n",
    "    unit_price = csr_matrix((df_inference[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X_inference = hstack([onehot_output, tfidf_output, unit_price], format=\"csr\")\n",
    "\n",
    "    result = fm_predictor.predict(X_inference.toarray())\n",
    "    preds = [i[\"score\"] for i in result[\"predictions\"]]\n",
    "    index_array = np.array(preds).argsort()\n",
    "    items = enc.inverse_transform(onehot_output)[:, 0]\n",
    "    top_recs = np.take_along_axis(items, index_array, axis=0)[: -n_recommendations - 1 : -1]\n",
    "    recommendations = [[i, item_map[i]] for i in top_recs]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04ca8ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['22423', 'REGENCY CAKESTAND 3 TIER'],\n",
       " ['85123A', 'WHITE HANGING HEART T-LIGHT HOLDER'],\n",
       " ['84879', 'ASSORTED COLOUR BIRD ORNAMENT'],\n",
       " ['47566', 'PARTY BUNTING'],\n",
       " ['23298', 'SPOTTY BUNTING']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 recommended products:\")\n",
    "get_recommendations(df, top_customer, n_recommendations=5, n_ranks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "977b9b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 customers with highest spend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['22423', 'REGENCY CAKESTAND 3 TIER'],\n",
       " ['22776', 'SWEETHEART CAKESTAND 3 TIER'],\n",
       " ['22624', 'IVORY KITCHEN SCALES'],\n",
       " ['85123A', 'WHITE HANGING HEART T-LIGHT HOLDER'],\n",
       " ['85099B', 'JUMBO BAG RED RETROSPOT'],\n",
       " ['21733', 'RED HANGING HEART T-LIGHT HOLDER'],\n",
       " ['22386', 'JUMBO BAG PINK POLKADOT'],\n",
       " ['84879', 'ASSORTED COLOUR BIRD ORNAMENT'],\n",
       " ['23084', 'RABBIT NIGHT LIGHT'],\n",
       " ['23199', 'JUMBO BAG APPLES']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 10 customers with highest spend\")\n",
    "get_recommendations(df, top_customer, n_recommendations=10, n_ranks=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928fc57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
